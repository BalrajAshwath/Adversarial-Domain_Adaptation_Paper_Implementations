{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implements WDGRL:\n",
    "- [Wasserstein Distance Guided Representation Learning, Shen et al. (2017)](https://arxiv.org/abs/1707.01217)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "![](model_archs/Wasserstein_Distance_Guided_Representation_Learning_for_Domain_Adaptation_model_arch.png)\n",
    "\n",
    "Image borrowed from [Wasserstein Distance Guided Representation Learning, Shen et al. (2017)](https://arxiv.org/abs/1707.01217)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import config\n",
    "from data import MNISTM\n",
    "from models import Net\n",
    "from utils import loop_iterable, set_requires_grad, GrayscaleToRgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDA-enabled GPU isn't found, we run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gradient_penalty` function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, h_s, h_t):\n",
    "    # based on: https://github.com/caogang/wgan-gp/blob/master/gan_cifar10.py#L116\n",
    "    alpha = torch.rand(h_s.size(0), 1).to(device)\n",
    "    differences = h_t - h_s\n",
    "    interpolates = h_s + (alpha * differences)\n",
    "    interpolates = torch.stack([interpolates, h_s, h_t]).requires_grad_()\n",
    "\n",
    "    preds = critic(interpolates)\n",
    "    gradients = grad(preds, interpolates,\n",
    "                     grad_outputs=torch.ones_like(preds),\n",
    "                     retain_graph=True, create_graph=True)[0]\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    gradient_penalty = ((gradient_norm - 1)**2).mean()\n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set necessary hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = 'trained_models/source.pt'\n",
    "batch_size = 64\n",
    "iterations = 500\n",
    "epochs = 5\n",
    "k_critic = 5\n",
    "k_clf = 1\n",
    "gamma = 10\n",
    "wd_clf = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train WDGRL model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 001: critic_loss=-6.0905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 002: critic_loss=-5.8040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 003: critic_loss=-2.1073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 004: critic_loss=-1.1040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 005: critic_loss=-0.7103\n"
     ]
    }
   ],
   "source": [
    "clf_model = Net().to(device)\n",
    "clf_model.load_state_dict(torch.load(MODEL_FILE))\n",
    "\n",
    "feature_extractor = clf_model.feature_extractor\n",
    "discriminator = clf_model.classifier\n",
    "\n",
    "critic = nn.Sequential(\n",
    "    nn.Linear(320, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ").to(device)\n",
    "\n",
    "half_batch = batch_size // 2\n",
    "source_dataset = MNIST(config.DATA_DIR/'mnist', train=True, download=True,\n",
    "                      transform=Compose([GrayscaleToRgb(), ToTensor()]))\n",
    "source_loader = DataLoader(source_dataset, batch_size=half_batch, drop_last=True,\n",
    "                           shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "target_dataset = MNISTM(train=False)\n",
    "target_loader = DataLoader(target_dataset, batch_size=half_batch, drop_last=True,\n",
    "                           shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "critic_optim = torch.optim.Adam(critic.parameters(), lr=1e-4)\n",
    "clf_optim = torch.optim.Adam(clf_model.parameters(), lr=1e-4)\n",
    "clf_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    batch_iterator = zip(loop_iterable(source_loader), loop_iterable(target_loader))\n",
    "\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for _ in trange(iterations, leave=False):\n",
    "        (source_x, source_y), (target_x, _) = next(batch_iterator)\n",
    "        # Train critic\n",
    "        set_requires_grad(feature_extractor, requires_grad=False)\n",
    "        set_requires_grad(critic, requires_grad=True)\n",
    "\n",
    "        source_x, target_x = source_x.to(device), target_x.to(device)\n",
    "        source_y = source_y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            h_s = feature_extractor(source_x).data.view(source_x.shape[0], -1)\n",
    "            h_t = feature_extractor(target_x).data.view(target_x.shape[0], -1)\n",
    "        for _ in range(k_critic):\n",
    "            gp = gradient_penalty(critic, h_s, h_t)\n",
    "\n",
    "            critic_s = critic(h_s)\n",
    "            critic_t = critic(h_t)\n",
    "            wasserstein_distance = critic_s.mean() - critic_t.mean()\n",
    "\n",
    "            critic_cost = -wasserstein_distance + gamma*gp\n",
    "\n",
    "            critic_optim.zero_grad()\n",
    "            critic_cost.backward()\n",
    "            critic_optim.step()\n",
    "\n",
    "            total_loss += critic_cost.item()\n",
    "\n",
    "        # Train classifier\n",
    "        set_requires_grad(feature_extractor, requires_grad=True)\n",
    "        set_requires_grad(critic, requires_grad=False)\n",
    "        for _ in range(k_clf):\n",
    "            source_features = feature_extractor(source_x).view(source_x.shape[0], -1)\n",
    "            target_features = feature_extractor(target_x).view(target_x.shape[0], -1)\n",
    "\n",
    "            source_preds = discriminator(source_features)\n",
    "            clf_loss = clf_criterion(source_preds, source_y)\n",
    "            wasserstein_distance = critic(source_features).mean() - critic(target_features).mean()\n",
    "\n",
    "            loss = clf_loss + wd_clf * wasserstein_distance\n",
    "            clf_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            clf_optim.step()\n",
    "\n",
    "    mean_loss = total_loss / (iterations * k_critic)\n",
    "    tqdm.write(f'EPOCH {epoch:03d}: critic_loss={mean_loss:.4f}')\n",
    "    torch.save(clf_model.state_dict(), 'trained_models/wdgrl.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
