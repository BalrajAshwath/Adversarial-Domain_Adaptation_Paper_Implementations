{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implements ADDA:\n",
    " - [Adversarial Discriminative Domain Adaptation, Tzeng et al. (2017)](https://arxiv.org/abs/1702.05464)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "![](model_archs/Adversarial_Discriminative_Domain_Adaptation_model_arch.jpg)\n",
    "Image borrowed from [Adversarial Discriminative Domain Adaptation, Tzeng et al. (2017)](https://arxiv.org/abs/1702.05464)\n",
    "\n",
    "Note: In the below code, source domain is MNIST and the target domain is MNIST-M unlike depicted in the figure above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, ToTensor\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import config\n",
    "from data import MNISTM\n",
    "from models import Net\n",
    "from utils import loop_iterable, set_requires_grad, GrayscaleToRgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If CUDA-enabled GPU isn't found, we run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set necessary hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_FILE = 'trained_models/source.pt'\n",
    "batch_size = 64\n",
    "iterations = 500\n",
    "epochs = 5\n",
    "k_disc = 1\n",
    "k_clf = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train ADDA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_model = Net().to(device)\n",
    "source_model.load_state_dict(torch.load(MODEL_FILE))\n",
    "source_model.eval()\n",
    "set_requires_grad(source_model, requires_grad=False)\n",
    "\n",
    "clf = source_model\n",
    "source_model = source_model.feature_extractor\n",
    "\n",
    "target_model = Net().to(device)\n",
    "target_model.load_state_dict(torch.load(MODEL_FILE))\n",
    "target_model = target_model.feature_extractor\n",
    "\n",
    "discriminator = nn.Sequential(\n",
    "    nn.Linear(320, 50),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(50, 20),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 1)\n",
    ").to(device)\n",
    "\n",
    "half_batch = batch_size // 2\n",
    "source_dataset = MNIST(config.DATA_DIR/'mnist', train=True, download=True,\n",
    "                      transform=Compose([GrayscaleToRgb(), ToTensor()]))\n",
    "source_loader = DataLoader(source_dataset, batch_size=half_batch,\n",
    "                           shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "target_dataset = MNISTM(train=False)\n",
    "target_loader = DataLoader(target_dataset, batch_size=half_batch,\n",
    "                           shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "discriminator_optim = torch.optim.Adam(discriminator.parameters())\n",
    "target_optim = torch.optim.Adam(target_model.parameters())\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    batch_iterator = zip(loop_iterable(source_loader), loop_iterable(target_loader))\n",
    "\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    for _ in trange(iterations, leave=False):\n",
    "        # Train discriminator\n",
    "        set_requires_grad(target_model, requires_grad=False)\n",
    "        set_requires_grad(discriminator, requires_grad=True)\n",
    "        for _ in range(k_disc):\n",
    "            (source_x, _), (target_x, _) = next(batch_iterator)\n",
    "            source_x, target_x = source_x.to(device), target_x.to(device)\n",
    "\n",
    "            source_features = source_model(source_x).view(source_x.shape[0], -1)\n",
    "            target_features = target_model(target_x).view(target_x.shape[0], -1)\n",
    "\n",
    "            discriminator_x = torch.cat([source_features, target_features])\n",
    "            discriminator_y = torch.cat([torch.ones(source_x.shape[0], device=device),\n",
    "                                         torch.zeros(target_x.shape[0], device=device)])\n",
    "\n",
    "            preds = discriminator(discriminator_x).squeeze()\n",
    "            loss = criterion(preds, discriminator_y)\n",
    "\n",
    "            discriminator_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            discriminator_optim.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_accuracy += ((preds > 0).long() == discriminator_y.long()).float().mean().item()\n",
    "\n",
    "        # Train classifier\n",
    "        set_requires_grad(target_model, requires_grad=True)\n",
    "        set_requires_grad(discriminator, requires_grad=False)\n",
    "        for _ in range(k_clf):\n",
    "            _, (target_x, _) = next(batch_iterator)\n",
    "            target_x = target_x.to(device)\n",
    "            target_features = target_model(target_x).view(target_x.shape[0], -1)\n",
    "\n",
    "            # flipped labels\n",
    "            discriminator_y = torch.ones(target_x.shape[0], device=device)\n",
    "\n",
    "            preds = discriminator(target_features).squeeze()\n",
    "            loss = criterion(preds, discriminator_y)\n",
    "\n",
    "            target_optim.zero_grad()\n",
    "            loss.backward()\n",
    "            target_optim.step()\n",
    "\n",
    "    mean_loss = total_loss / (iterations*k_disc)\n",
    "    mean_accuracy = total_accuracy / (iterations*k_disc)\n",
    "    tqdm.write(f'EPOCH {epoch:03d}: discriminator_loss={mean_loss:.4f}, '\n",
    "               f'discriminator_accuracy={mean_accuracy:.4f}')\n",
    "\n",
    "    # Create the full target model and save it\n",
    "    clf.feature_extractor = target_model\n",
    "    torch.save(clf.state_dict(), 'trained_models/adda.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
